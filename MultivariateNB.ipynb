{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultivariateNB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW1P5zkj9EVS"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJHQAG8P93wE"
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection', sep='\\t',header=None, names=['Type', 'Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xd80yiTu-FcV",
        "outputId": "661b4d54-ab30-46a3-ff8c-140b1657e41e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type                                               Text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vETPA1D_sb1",
        "outputId": "93af1b7f-37c6-45ce-8464-84351f8015c6"
      },
      "source": [
        "df['Type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V1IhqwPANjN",
        "outputId": "46ea294e-cfc2-4a1c-ee23-2f8638e36a79"
      },
      "source": [
        "df['Type'].value_counts()*100/df['Type'].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     86.593683\n",
              "spam    13.406317\n",
              "Name: Type, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKyeiTgFApRl"
      },
      "source": [
        "df['Text'] = df['Text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wCEZTbacBOqV",
        "outputId": "65337485-0968-42aa-ff1b-4c1a64893415"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type                                               Text\n",
              "0   ham  go until jurong point, crazy.. available only ...\n",
              "1   ham                      ok lar... joking wif u oni...\n",
              "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
              "3   ham  u dun say so early hor... u c already then say...\n",
              "4   ham  nah i don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mXmPvek1Eedd",
        "outputId": "231e5785-8a3e-4f21-8626-c8d05f99901f"
      },
      "source": [
        "#Remove punctuations\n",
        "df['Text'] = df.Text.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5APO21Nghj"
      },
      "source": [
        "#Remove numbers\n",
        "df['Text'] = df['Text'].str.replace('\\d+', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8BiB4GgCCXh",
        "outputId": "7af64f9e-5abf-4dc9-bb1e-27bea05706c9"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "goK0jNgSFbTf",
        "outputId": "5321358f-640c-4617-96c8-9b4c307c8f0a"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "df['Text'] = df['Text'].apply(lambda x: ''.join([stemmer.stem(y) for y in x]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry in  a wkly comp to win fa cup final tkts st may  text fa to  to receive entry questionstd txt ratetcs apply overs'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCPZfSo8BQSl",
        "outputId": "a3c3439d-a766-4837-c840-298f9b8f422c"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "A6wUa_rGBs3t",
        "outputId": "cba415f1-d717-4cec-ef00-ef5a582aa4cb"
      },
      "source": [
        "#Stop word removal\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply overs'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FB1UPs0mH0Ta",
        "outputId": "4af0b801-e3bb-44f9-e8dd-dea436fc000d"
      },
      "source": [
        "#Lemmatization\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in w_tokenizer.tokenize(x) ]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply over'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nTbJBepCb_N"
      },
      "source": [
        "#shuffle all the data\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dUZ89Cu7KN1o",
        "outputId": "ee971d51-5a64-48b2-ecb7-5d8333c8ea77"
      },
      "source": [
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'didt play one day last year know even though good team like india'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImQUaJZ5ZRvO"
      },
      "source": [
        "def calculateSpamProbability(index):\n",
        "  sent = str(test_df.iloc[index]['Text']).split(' ')\n",
        "  p=1\n",
        "  for word in sent:\n",
        "    n_wi_spam = 0\n",
        "    for message in spam_messages:\n",
        "      if word in message:\n",
        "        n_wi_spam = n_wi_spam + 1\n",
        "    p = p * ((n_wi_spam + alpha)/(N_spam + alpha*N_vocabulary))\n",
        "  p = p*P_spam\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgfjUk22ZSsi"
      },
      "source": [
        "def calculateHamProbability(index):\n",
        "  sent = str(test_df.iloc[index]['Text']).split(' ')\n",
        "  p=1\n",
        "  for word in sent:\n",
        "    n_wi_ham = 0\n",
        "    for message in ham_messages:\n",
        "      if word in message:\n",
        "        n_wi_ham = n_wi_ham + 1\n",
        "    p = p * ((n_wi_ham + alpha)/(N_ham + alpha*N_vocabulary))\n",
        "  p = p*P_ham\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCUR7dcKQsv",
        "outputId": "8a2cfca3-7fe5-4ade-8bf5-0767af4771d8"
      },
      "source": [
        "#split in training and testing set, 20% test and 80% train data, use k fold cross validation\n",
        "k=0\n",
        "start_index_test = [0,0.2,0.4,0.6,0.8]\n",
        "end_index_test  = [0.2,0.4,0.6,0.8,1]  \n",
        "combined_accuracy = []\n",
        "while(k<5):\n",
        "  print(\"\\n***********K={}***********\".format(k+1))\n",
        "  start_index = start_index_test[k] * df['Type'].count()\n",
        "  start_index = int(start_index)\n",
        "  end_index = end_index_test[k] * df['Type'].count()\n",
        "  end_index = int(end_index)\n",
        "  train = []\n",
        "  test= []\n",
        "\n",
        "  for i in range(start_index,end_index):\n",
        "    test.append([df.iloc[i]['Type'], df.iloc[i]['Text']])\n",
        "\n",
        "  for i in range(0,df['Type'].count()):\n",
        "    if i not in range(start_index,end_index):\n",
        "      train.append([df.iloc[i]['Type'], df.iloc[i]['Text']])\n",
        "\n",
        "  test_df = pd.DataFrame(test, columns=['Type', 'Text'])\n",
        "  train_df = pd.DataFrame(train, columns=['Type', 'Text'])\n",
        "  \n",
        "  #creating a vocabulary from test set\n",
        "  vocabulary = []\n",
        "  sent = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    sent = str(train_df.iloc[i]['Text']).split(' ')\n",
        "    for word in sent:\n",
        "      if word not in vocabulary:\n",
        "        vocabulary.append(word)\n",
        "  print('Length of vocabulary {}'.format(len(vocabulary)))\n",
        "\n",
        "  #add word count to each row\n",
        "  combinedList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    newList =[]\n",
        "    sent = str(train_df.iloc[i]['Text']).split(' ')\n",
        "    for word in vocabulary:\n",
        "      newList.append(sent.count(word))\n",
        "    combinedList.append(newList)\n",
        "  train_df['WordCount'] = combinedList\n",
        "  print(train_df.head())\n",
        "\n",
        "  #Calculate Pspam and Pham\n",
        "  p = list(train_df['Type'].value_counts()/train_df['Type'].count())\n",
        "  P_ham = p[0]\n",
        "  P_spam = p[1]\n",
        "\n",
        "  #Calculate No of words in spam messages:\n",
        "  spamList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    if train_df.iloc[i]['Type'] == 'spam':\n",
        "      spamList.append( str(train_df.iloc[i]['Text']).split(' '))\n",
        "  N_spam = len(spamList)\n",
        "\n",
        "  #Calculate No of words in ham messages:\n",
        "  spamList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    if train_df.iloc[i]['Type'] == 'ham':\n",
        "      spamList.append( str(train_df.iloc[i]['Text']).split(' '))\n",
        "  N_ham = len(spamList)\n",
        "\n",
        "  N_vocabulary = len(vocabulary)\n",
        "\n",
        "  # Laplace smoothing\n",
        "  alpha = 1\n",
        "\n",
        "  #Get all words in spam and ham meassages respectively\n",
        "  spam_messages = []\n",
        "  ham_messages  = []\n",
        "  for index, row in train_df.iterrows():\n",
        "    if row['Type'] == 'spam':\n",
        "      spam_messages.append(row['Text'])\n",
        "    else:\n",
        "      ham_messages.append(row['Text'])\n",
        "\n",
        "  #Testing NB\n",
        "  category = []\n",
        "  for i in range(0,test_df['Type'].count()):\n",
        "    if calculateSpamProbability(i)<calculateHamProbability(i):\n",
        "      category.append('ham')\n",
        "    else:\n",
        "      category.append('spam')\n",
        "  test_df['TestCategory'] = category\n",
        "  print(test_df)\n",
        "  nCorrect = 0\n",
        "  for i in range(0,test_df['Type'].count()):\n",
        "    if test_df.iloc[i]['Type'] ==  test_df.iloc[i]['TestCategory']:\n",
        "      nCorrect = nCorrect+1\n",
        "  accuracy = nCorrect/test_df['Type'].count()\n",
        "  combined_accuracy.append(accuracy)\n",
        "  print('Accuraccy of Multinomial NB is {}'.format(accuracy))\n",
        "  k=k+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***********K=1***********\n",
            "Length of vocabulary 7057\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "     Type                                               Text TestCategory\n",
            "0     ham                               sister going earn da          ham\n",
            "1     ham  true passable get high score apply phd get yea...          ham\n",
            "2     ham  didt play one day last year know even though g...          ham\n",
            "3     ham  u ned convince tht possible witot hurting feel...          ham\n",
            "4     ham         dad gonna call get work ask crazy question          ham\n",
            "...   ...                                                ...          ...\n",
            "1109  ham             im sure checking happening around area          ham\n",
            "1110  ham                              friend u stay fb chat          ham\n",
            "1111  ham                                          dear call          ham\n",
            "1112  ham           wasnt well babe swollen gland throat end          ham\n",
            "1113  ham     short cute good person dont try prove gud noon          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9614003590664273\n",
            "\n",
            "***********K=2***********\n",
            "Length of vocabulary 6956\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  ham  ...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "     Type                                               Text TestCategory\n",
            "0     ham                send email mind ltgt time per night          ham\n",
            "1     ham  twenty past five said train durham already coz...          ham\n",
            "2     ham                                   mum went dentist          ham\n",
            "3     ham                            come online today night          ham\n",
            "4     ham             still chance search hard get itlet try          ham\n",
            "...   ...                                                ...          ...\n",
            "1109  ham         r u yet im wearing blue shirt n black pant          ham\n",
            "1110  ham                              anything lor u decide          ham\n",
            "1111  ham                 kkim also finewhen complete course          ham\n",
            "1112  ham                        im freezing craving ice fml          ham\n",
            "1113  ham                         depends would like treated          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9712746858168761\n",
            "\n",
            "***********K=3***********\n",
            "Length of vocabulary 7081\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  ham  ...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "      Type                                               Text TestCategory\n",
            "0      ham                   directly behind abt row behind Ã¼          ham\n",
            "1      ham                            hey inconvenient si huh          ham\n",
            "2      ham                                  wish u feel alone          ham\n",
            "3      ham                               birthday feb ltgt da          ham\n",
            "4      ham  jus reached home go bathe first si using net t...          ham\n",
            "...    ...                                                ...          ...\n",
            "1110   ham  affidavit say ltgt e twiggs st division g cour...          ham\n",
            "1111   ham  hi spoke maneesha v wed like know satisfied ex...          ham\n",
            "1112   ham                          armand say get as epsilon          ham\n",
            "1113   ham                                  curious cuz asked          ham\n",
            "1114  spam  download many ringtones u like restriction cho...          ham\n",
            "\n",
            "[1115 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9739910313901345\n",
            "\n",
            "***********K=4***********\n",
            "Length of vocabulary 7013\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  ham  ...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "     Type                                               Text TestCategory\n",
            "0     ham  like made throw smoking friend car one time aw...          ham\n",
            "1     ham                                      new year plan          ham\n",
            "2     ham  nvm im going wear sport shoe anyway im going l...          ham\n",
            "3     ham                   seems unnecessarily affectionate          ham\n",
            "4     ham  come life brought sun shiny warming heart putt...          ham\n",
            "...   ...                                                ...          ...\n",
            "1109  ham            think two still need get cash def ready          ham\n",
            "1110  ham  sister belongs family hope tomorrow pray herwh...          ham\n",
            "1111  ham                               horrible bf v hungry          ham\n",
            "1112  ham                               sorry ill call later          ham\n",
            "1113  ham                    hospital da return home evening          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9766606822262118\n",
            "\n",
            "***********K=5***********\n",
            "Length of vocabulary 7117\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4  ham  ...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "      Type                                               Text TestCategory\n",
            "0      ham                      u wan haf lunch im da canteen          ham\n",
            "1      ham                               got back dislike bed          ham\n",
            "2      ham                sorry roommate took forever ok come          ham\n",
            "3     spam  urgent important information user today lucky ...         spam\n",
            "4      ham                         yahoo boy bring perf legal          ham\n",
            "...    ...                                                ...          ...\n",
            "1110   ham  dont plan staying night prolly wont back til late          ham\n",
            "1111   ham       dun need use dial juz open da browser n surf          ham\n",
            "1112   ham                               nope im still market          ham\n",
            "1113   ham  send ur friend receive something ur voice spea...          ham\n",
            "1114   ham  know account detailsi ask mom send youmy mom r...          ham\n",
            "\n",
            "[1115 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9748878923766816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU6Xpgaiy7DB",
        "outputId": "362682ec-bf5d-45eb-cd58-8bf9f7d02d64"
      },
      "source": [
        "print(combined_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9614003590664273, 0.9712746858168761, 0.9739910313901345, 0.9766606822262118, 0.9748878923766816]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFDsxIVTbKyQ",
        "outputId": "b53529a5-59c5-46b9-e9b4-62e5e8f6e262"
      },
      "source": [
        "print(\"Average accuraccy {}\".format(sum(combined_accuracy)/len(combined_accuracy)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuraccy 0.9716429301752662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yMS7iRbbfCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}