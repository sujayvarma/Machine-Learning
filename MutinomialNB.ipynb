{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MutinomialNB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW1P5zkj9EVS"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJHQAG8P93wE"
      },
      "source": [
        "df = pd.read_csv('SMSSpamCollection', sep='\\t',header=None, names=['Type', 'Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xd80yiTu-FcV",
        "outputId": "d75d5f2f-c0ff-4be9-f034-39be094aeefc"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type                                               Text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vETPA1D_sb1",
        "outputId": "e8af5cfa-b63f-454b-87ad-68514d5883df"
      },
      "source": [
        "df['Type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     4825\n",
              "spam     747\n",
              "Name: Type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V1IhqwPANjN",
        "outputId": "ab55a2f6-3389-4e04-c77a-36a3be44aee0"
      },
      "source": [
        "df['Type'].value_counts()*100/df['Type'].count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     86.593683\n",
              "spam    13.406317\n",
              "Name: Type, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKyeiTgFApRl"
      },
      "source": [
        "df['Text'] = df['Text'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wCEZTbacBOqV",
        "outputId": "9db37ba2-1047-436b-ee8c-88767e8b8eb8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>go until jurong point, crazy.. available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>ok lar... joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>u dun say so early hor... u c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Type                                               Text\n",
              "0   ham  go until jurong point, crazy.. available only ...\n",
              "1   ham                      ok lar... joking wif u oni...\n",
              "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
              "3   ham  u dun say so early hor... u c already then say...\n",
              "4   ham  nah i don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mXmPvek1Eedd",
        "outputId": "121c147c-ee38-4caf-c1ee-e66afb2da796"
      },
      "source": [
        "#Remove punctuations\n",
        "df['Text'] = df.Text.str.replace(\"[^\\w\\s]\", \"\").str.lower()\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5APO21Nghj"
      },
      "source": [
        "#Remove numbers\n",
        "df['Text'] = df['Text'].str.replace('\\d+', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8BiB4GgCCXh",
        "outputId": "8647abd9-423f-4d1f-96e8-0732f9e53bcb"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "goK0jNgSFbTf",
        "outputId": "12c67b51-5c77-4338-baf6-8a38e7a90aa9"
      },
      "source": [
        "#Stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "df['Text'] = df['Text'].apply(lambda x: ''.join([stemmer.stem(y) for y in x]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry in  a wkly comp to win fa cup final tkts st may  text fa to  to receive entry questionstd txt ratetcs apply overs'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCPZfSo8BQSl",
        "outputId": "a22ad879-fcd9-4e9a-a375-9acaa515e65d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "A6wUa_rGBs3t",
        "outputId": "8c1539e9-a788-4585-ed7a-e337a4209f07"
      },
      "source": [
        "#Stop word removal\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply overs'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FB1UPs0mH0Ta",
        "outputId": "48a5fc44-97f8-409b-bd3d-db47e992f77b"
      },
      "source": [
        "#Lemmatization\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "df['Text'] = df['Text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(y) for y in w_tokenizer.tokenize(x) ]))\n",
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'free entry wkly comp win fa cup final tkts st may text fa receive entry questionstd txt ratetcs apply over'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nTbJBepCb_N"
      },
      "source": [
        "#shuffle all the data\n",
        "df = df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dUZ89Cu7KN1o",
        "outputId": "fcace069-c6cc-4649-9a02-0621fbe4a695"
      },
      "source": [
        "df.iloc[2]['Text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hey happy birthday'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImQUaJZ5ZRvO"
      },
      "source": [
        "def calculateSpamProbability(index):\n",
        "  sent = str(test_df.iloc[index]['Text']).split(' ')\n",
        "  p=1\n",
        "  for word in sent:\n",
        "    n_wi_spam = 0\n",
        "    n_wi_spam = n_wi_spam + spam_messages.count(word)\n",
        "    p = p * ((n_wi_spam + alpha)/(N_spam + alpha*N_vocabulary))\n",
        "  p = p*P_spam\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgfjUk22ZSsi"
      },
      "source": [
        "def calculateHamProbability(index):\n",
        "  sent = str(test_df.iloc[index]['Text']).split(' ')\n",
        "  p=1\n",
        "  for word in sent:\n",
        "    n_wi_ham = 0\n",
        "    n_wi_ham = n_wi_ham + ham_messages.count(word)\n",
        "    p = p * ((n_wi_ham + alpha)/(N_ham + alpha*N_vocabulary))\n",
        "  p = p*P_ham\n",
        "  return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lCUR7dcKQsv",
        "outputId": "ec27ce27-eb56-41be-9d75-9e98ca1639bf"
      },
      "source": [
        "#split in training and testing set, 20% test and 80% train data, use k fold cross validation\n",
        "k=0\n",
        "start_index_test = [0,0.2,0.4,0.6,0.8]\n",
        "end_index_test  = [0.2,0.4,0.6,0.8,1]  \n",
        "combined_accuracy = []\n",
        "while(k<5):\n",
        "  print(\"\\n***********K={}***********\".format(k+1))\n",
        "  start_index = start_index_test[k] * df['Type'].count()\n",
        "  start_index = int(start_index)\n",
        "  end_index = end_index_test[k] * df['Type'].count()\n",
        "  end_index = int(end_index)\n",
        "  train = []\n",
        "  test= []\n",
        "\n",
        "  for i in range(start_index,end_index):\n",
        "    test.append([df.iloc[i]['Type'], df.iloc[i]['Text']])\n",
        "\n",
        "  for i in range(0,df['Type'].count()):\n",
        "    if i not in range(start_index,end_index):\n",
        "      train.append([df.iloc[i]['Type'], df.iloc[i]['Text']])\n",
        "\n",
        "  test_df = pd.DataFrame(test, columns=['Type', 'Text'])\n",
        "  train_df = pd.DataFrame(train, columns=['Type', 'Text'])\n",
        "  \n",
        "  #creating a vocabulary from test set\n",
        "  vocabulary = []\n",
        "  sent = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    sent = str(train_df.iloc[i]['Text']).split(' ')\n",
        "    for word in sent:\n",
        "      if word not in vocabulary:\n",
        "        vocabulary.append(word)\n",
        "  print('Length of vocabulary {}'.format(len(vocabulary)))\n",
        "\n",
        "  #add word count to each row\n",
        "  combinedList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    newList =[]\n",
        "    sent = str(train_df.iloc[i]['Text']).split(' ')\n",
        "    for word in vocabulary:\n",
        "      newList.append(sent.count(word))\n",
        "    combinedList.append(newList)\n",
        "  train_df['WordCount'] = combinedList\n",
        "  print(train_df.head())\n",
        "\n",
        "  #Calculate Pspam and Pham\n",
        "  p = list(train_df['Type'].value_counts()/train_df['Type'].count())\n",
        "  P_ham = p[0]\n",
        "  P_spam = p[1]\n",
        "\n",
        "  #Calculate No of words in spam messages:\n",
        "  spamList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    if train_df.iloc[i]['Type'] == 'spam':\n",
        "      spamList.append( str(train_df.iloc[i]['Text']).split(' '))\n",
        "  N_spam = len(spamList)\n",
        "\n",
        "  #Calculate No of words in ham messages:\n",
        "  spamList = []\n",
        "  for i in range(0,train_df['Type'].count()):\n",
        "    if train_df.iloc[i]['Type'] == 'ham':\n",
        "      spamList.append( str(train_df.iloc[i]['Text']).split(' '))\n",
        "  N_ham = len(spamList)\n",
        "\n",
        "  N_vocabulary = len(vocabulary)\n",
        "\n",
        "  # Laplace smoothing\n",
        "  alpha = 1\n",
        "\n",
        "  #Get all words in spam and ham meassages respectively\n",
        "  spam_messages = []\n",
        "  ham_messages  = []\n",
        "  for index, row in train_df.iterrows():\n",
        "    if row['Type'] == 'spam':\n",
        "      spam_messages = spam_messages + str(row['Text']).split(' ')\n",
        "    else:\n",
        "      ham_messages = ham_messages + str(row['Text']).split(' ')\n",
        "\n",
        "  #Testing NB\n",
        "  category = []\n",
        "  for i in range(0,test_df['Type'].count()):\n",
        "    if calculateSpamProbability(i)<calculateHamProbability(i):\n",
        "      category.append('ham')\n",
        "    else:\n",
        "      category.append('spam')\n",
        "  test_df['TestCategory'] = category\n",
        "  print(test_df)\n",
        "  nCorrect = 0\n",
        "  for i in range(0,test_df['Type'].count()):\n",
        "    if test_df.iloc[i]['Type'] ==  test_df.iloc[i]['TestCategory']:\n",
        "      nCorrect = nCorrect+1\n",
        "  accuracy = nCorrect/test_df['Type'].count()\n",
        "  combined_accuracy.append(accuracy)\n",
        "  print('Accuraccy of Multinomial NB is {}'.format(accuracy))\n",
        "  k=k+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***********K=1***********\n",
            "Length of vocabulary 7091\n",
            "   Type  ...                                          WordCount\n",
            "0  spam  ...  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...\n",
            "1   ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...\n",
            "2   ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "3   ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "4   ham  ...  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "     Type                                               Text TestCategory\n",
            "0     ham                               nottel software name          ham\n",
            "1     ham      like think there always possibility pub later          ham\n",
            "2     ham                                 hey happy birthday          ham\n",
            "3     ham                                         u wana see          ham\n",
            "4     ham  impossible argue always treat like sub like ne...          ham\n",
            "...   ...                                                ...          ...\n",
            "1109  ham                                             yup ok          ham\n",
            "1110  ham  didnt mean post wrote like many time ive ritte...          ham\n",
            "1111  ham                         thats fine ill bitch later          ham\n",
            "1112  ham                     dont know shes getting message          ham\n",
            "1113  ham  cant feel nauseous im pissed didnt eat sweet w...          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9757630161579892\n",
            "\n",
            "***********K=2***********\n",
            "Length of vocabulary 7069\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
            "4  ham  ...  [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "      Type                                               Text TestCategory\n",
            "0     spam  congratulation week competition draw u prize c...         spam\n",
            "1      ham                                outside office take          ham\n",
            "2      ham        message truro hospital ext phone phone side          ham\n",
            "3      ham                                  dun wear jean lor          ham\n",
            "4      ham  grandma oh dear u still ill felt shit morning ...          ham\n",
            "...    ...                                                ...          ...\n",
            "1109   ham                                        k k sm chat          ham\n",
            "1110   ham                 come mu sorting narcotic situation          ham\n",
            "1111   ham                           want finally lunch today          ham\n",
            "1112   ham                                      way office da          ham\n",
            "1113   ham                     studying ill free next weekend          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9721723518850988\n",
            "\n",
            "***********K=3***********\n",
            "Length of vocabulary 6991\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
            "4  ham  ...  [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "      Type                                               Text TestCategory\n",
            "0      ham  hey glad u r better hear u treated urself digi...          ham\n",
            "1      ham  becoz ltgt jan whn al post ofice holiday cn go...          ham\n",
            "2      ham  fighting world easy u either win lose bt fight...          ham\n",
            "3      ham              co darren say ü considering mah ask ü          ham\n",
            "4     spam  u subscribed best mobile content service uk pe...         spam\n",
            "...    ...                                                ...          ...\n",
            "1110   ham  beautiful truth gravity read carefully heart f...          ham\n",
            "1111   ham  told tell stupid hear wont tell anything dad c...          ham\n",
            "1112   ham                                  need drug anymore          ham\n",
            "1113   ham                    black shirt n blue jean thk c ü          ham\n",
            "1114   ham               either idea know anyplaces something          ham\n",
            "\n",
            "[1115 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9766816143497757\n",
            "\n",
            "***********K=4***********\n",
            "Length of vocabulary 7051\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
            "4  ham  ...  [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "      Type                                               Text TestCategory\n",
            "0      ham                 ltgt thats guess thats easy enough          ham\n",
            "1      ham  tmr ü brin lar aiya later come n c lar mayb ü ...          ham\n",
            "2      ham                        change e one next escalator          ham\n",
            "3      ham  okay name ur price long legal wen pick u ave x...          ham\n",
            "4      ham                                              thanx          ham\n",
            "...    ...                                                ...          ...\n",
            "1109  spam  wanna get laid nite want real dogging location...         spam\n",
            "1110   ham      gonna blake night might able get little early          ham\n",
            "1111   ham             slept thinkthis time ltgt pm dangerous          ham\n",
            "1112   ham          please dont text anymore nothing else say          ham\n",
            "1113   ham       nice talking please dont forget pix want see          ham\n",
            "\n",
            "[1114 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.9802513464991023\n",
            "\n",
            "***********K=5***********\n",
            "Length of vocabulary 7033\n",
            "  Type  ...                                          WordCount\n",
            "0  ham  ...  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "1  ham  ...  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
            "2  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, ...\n",
            "3  ham  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
            "4  ham  ...  [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "     Type                                               Text TestCategory\n",
            "0     ham  sorry battery died come im getting gram wheres...          ham\n",
            "1     ham                               oh okie lorwe go sat          ham\n",
            "2     ham                              big god bring success          ham\n",
            "3     ham                                   want ltgt r dado          ham\n",
            "4     ham                                  keeping away like          ham\n",
            "...   ...                                                ...          ...\n",
            "1110  ham                                  happen tell truth          ham\n",
            "1111  ham                                   yes innocent fun          ham\n",
            "1112  ham                                    take exam march          ham\n",
            "1113  ham  sorry left phone upstairs ok might hectic woul...          ham\n",
            "1114  ham  miss much im desparate recorded message left d...          ham\n",
            "\n",
            "[1115 rows x 3 columns]\n",
            "Accuraccy of Multinomial NB is 0.979372197309417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU6Xpgaiy7DB",
        "outputId": "d411c96a-1edb-4e58-f3e6-054710604db6"
      },
      "source": [
        "print(combined_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9757630161579892, 0.9721723518850988, 0.9766816143497757, 0.9802513464991023, 0.979372197309417]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFDsxIVTbKyQ",
        "outputId": "bd7e2392-e597-4ac8-c64d-e44f251a705a"
      },
      "source": [
        "print(\"Average accuraccy {}\".format(sum(combined_accuracy)/len(combined_accuracy)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuraccy 0.9768481052402767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yMS7iRbbfCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}